[
{
	"uri": "/1-setup/",
	"title": "Account and Environment Setup",
	"tags": [],
	"description": "",
	"content": "Select Region We will only use one region throughout the workshop and in this workshop we will use ap-southeast-1 (Asia Pacific - Singapore).\nYou can choose another region depending on where you do this workshop to receive faster feedback.\nInitialize AWS Cloud9 IDE AWS Cloud9 is a cloud-based integrated development environment (IDE) that allows you to write, run, and debug code in the browser. It includes a code editor, debugger and terminal. Cloud9 comes pre-packaged with the necessary tools for popular programming languages and AWS Command Line Interface (CLI) is pre-installed so you don\u0026rsquo;t need to install files or configure it. for the device you are using.\nYour Cloud9 environment will have access to the same AWS resources as the user you logged into the AWS Management Console.\nSetup Cloud9 After logging in to the AWS Management Console with your account, go to Cloud9 console. Click on the Create environment button.\nNext, enter cost-optimization-workshop for the value of Name, choose to create a new EC2 in the Environment type section with Instance type as **t2.micro **. Leave the rest as default, finally click on the Create button.\nIt will take a few minutes for the Cloud9 Environment to be created.\nSetup Cloud9 IDE When Cloud9 IDE is successfully created, click on the Open button as shown on the screen.\nAnd the Cloud9 IDE interface is opened, close the Welcome tab, close the pre-initialized terminal and open a new terminal tab.\nYou can now run AWS CLI commands on the Cloud9 IDE terminal without having to reinstall it. Please keep the AWS Cloud9 IDE open in your browser throughout this workshop.\nNext, you need to authenticate the account currently accessing this Cloud9 IDE by pasting the following command into the newly created terminal aws sts get-caller-indentity and press Enter. You will see output as shown below.\nResize Cloud9 Environment Before starting implementation, you need to resize the current storage by running the command below in Cloud9 terminal. First we need to download the resize.sh file from the link below using the curl command.\ncurl \u0026#39;https://static.us-east-1.prod.workshops.aws/public/9cac7f06-a925-4a51-99f6-38ec4fb8707c/static/code/resize.sh\u0026#39; -o resize.sh When the download is successful, we will receive the image below.\nNow we will run the resize.sh script file with the bash command.\nbash resize.sh 20 And we will receive the results as shown after successful resizing.\nTo read and understand more about the resize.sh script, please visit the documentation here Cloud9 documentation\n"
},
{
	"uri": "/4-traffic/4.1-deploy/",
	"title": "Deploy",
	"tags": [],
	"description": "",
	"content": "First, in your Cloud9 workspace, move into environment.\ncd ~/environment/ Use the command below to download the artifact code.\ncurl \u0026#39;https://static.us-east-1.prod.workshops.aws/public/9cac7f06-a925-4a51-99f6-38ec4fb8707c/static/code/traffic-throttle.zip\u0026#39; -o traffic-throttle.zip \u0026amp;\u0026amp; unzip traffic-throttle.zip After downloading, you will see a new folder called traffic-throttle.\nMove into this folder.\ncd traffic-throttle Next, you will package and deploy the SAM application with the command:\nsam build \u0026amp;\u0026amp; sam deploy --guided Stack Name: traffic-throttle AWS Region: ap-southeast-1 Confirm changes before deploy [y/N]: default Allow SAM CLI IAM role creation [Y/n]: default Disable rollback: default Save arguments to configuration file [Y/n]: default SAM configuration file [samconfig.toml]: default SAM configuration environment [default]: default After successful deployment, we will receive a confirmation line as below.\nSuccessfully created/updated stack - traffic-throttle in ap-southeast-1 "
},
{
	"uri": "/2-powertuning/2.1-deploytuning/",
	"title": "Deploy The Lambda Power Tuning Tool",
	"tags": [],
	"description": "",
	"content": "Lambda Power Tuning Tool is stored in Serverless Application Repository. SAR allows customers to quickly publish and deploy Serverless applications. Customers can configure SAR permissions to allow access to specific accounts, accounts within an organization, or public access to potentially launch applications.\nLaunch Lambda Power Tuning Tool First, please visit Serverless Application Repository. In the left sidebar, select Available applications. On the application search section, enter AWS Lambda Power Tuning in the Public applications section and select Show apps that create custom IAM roles or resource policies. Then select the aws-lambda-power-tuning application.\nOn the Review, configure and deploy page, we leave the parameters as default, select I acknowledge that this app creates custom IAM roles and click the Deploy button at the bottom of the page.\nWait a few minutes after the application is deployed, we go to CloudFormation Console to check whether the application has been installed successfully or not.\n"
},
{
	"uri": "/3-graviton2/3.1-deploytuning/",
	"title": "Deploy The Lambda Power Tuning Tool",
	"tags": [],
	"description": "",
	"content": "We will start by creating a Lambda function written in Python and deployed on x86 architecture. We will then convert this function to the Arm-based Graviton2 architecture. We will use the Lambda Power Tuning tool from Module 1.\nLambda Power Tuning Tool is hosted on Serverless Application Repository. Serverless Application Repository (SAR) allows customers to quickly publish and deploy serverless applications. Customers can configure SAR permission to allow access to specific accounts, accounts within an organization, or public access to potentially launch applications. Visit for more information about Serverless Application Repository here\nPlease review the step Deploy Lambda Power Tuning Tool in Module 1 for more details on how to deploy Lambda Power Tuning Tool.\n"
},
{
	"uri": "/",
	"title": "Optimize for Serverless (Performance and Cost)",
	"tags": [],
	"description": "",
	"content": "Optimize for Serverless (Performance and Cost) Introduction In this workshop, we will learn ways to optimize performance and cost of Serverless.\nTo see more Serverless Architecture Best Practices, please visit Serverless Application Lens to find the words AWS Well Architected Framework.\nThis workshop will consist of sections surrounding a common goal of cost reduction. All modules are self-contained so you can complete each section separately.\nThe modules are arranged in the order of easiest implementation and there are 3 main sections in this workshop.\nPower Tuning and Log Tuning are best practices that are often overlooked, and implementing them does not impact the performance of your application. In this workshop, you will go into implementing the Power Tuning method.\nGravition2, Direct Integration, Provisioned Concurrency and Code Tuning are configuration-based approaches that require some code touches and operation overhead. In this workshop, you will practice the Gravition2 method.\nTraffic Throttling and Asynchronous Workflows require an overall design change to your Serverless application. And we\u0026rsquo;ll go do Traffic Throttling.\nWhat you will learn Throughout this workshop, you will learn some best practice techniques for optimizing Serverless workloads to reduce costs and increase performance. This workshop focuses on AWS Lambda, but other services used include:\nAmazon SQS Amazon API Gateway Amazon DynamoDB AWS Step Functions AWS AppConfig Knowledge and skills needed to complete this workshop Basic knowledge of AWS Serverless services required including AWS Lambda, API Gateway, SQS and DynamoDB. You must be familiar with AWS Console, AWS CLI, AWS IAM and CloudFormation.\nBasic knowledge of Linux and Python is also an advantage.\nContent Account and Environment Setup Power Tuning Graviton2 Traffic Throttling Cleanup "
},
{
	"uri": "/2-powertuning/2.2-deploytest/",
	"title": "Deploy Test Lambda Function",
	"tags": [],
	"description": "",
	"content": "Now that you have installed the power tuning tool, the next step is to run the AWS Lambda function based on that tool. In this section, you will deploy the Lambda function to test the use of the Power Tuning Tool. This function consumes a lot of memory, uses matrices.\nDeploy the function Make sure you are deploying this function in the same region where you installed the Lambda Power Tuning Tool\nFirst, go to AWS Lambda Console, look at the left sidebar and select Functions. Click on the Create function button.\nOn the Create function page, select Author from scratch, name the Function name lambda-power-tuning-test. Select Runtime as Python 3.8 and Architecture as x86_64, leave the rest as default. Finally, click on the Create function button at the bottom of the page. Wait a moment, when the function is created, you will be navigated to that function. In the Function overview section, click on Layers, it will move us to the bottom of the page in the Layers section.\nClick on the Add a layer button in the right corner.\nOn the Add layer page, select AWS layers for the Layer source section. In the AWS layers section, select AWSLambda-Python38-SciPy1x and the latest Version at the time you created it. Finally, click on the Add button at the bottom of the page.\nWait a moment when the new layer is added to the Lambda function, go down to the Code section and copy the code below and paste it into the lambda_function.py file.\nimport json\rimport numpy as np\rfrom scipy.spatial import ConvexHull\rdef lambda_handler(event, context):\rms = 100\rprint(\u0026#34;Printing from version on 10302020 - size of matrix\u0026#34;, ms,\u0026#34;x\u0026#34;,ms)\rprint(\u0026#34;\\nFilling the matrix with random integers below 100\\n\u0026#34;)\rmatrix_a = np.random.randint(100, size=(ms, ms))\rprint(matrix_a)\rprint(\u0026#34;random matrix_b =\u0026#34;)\rmatrix_b = np.random.randint(100, size=(ms, ms))\rprint(matrix_b)\rprint(\u0026#34;matrix_a * matrix_b = \u0026#34;)\rprint(matrix_a.dot(matrix_b))\rnum_points = 10\rprint(num_points, \u0026#34;random points:\u0026#34;)\rpoints = np.random.rand(num_points, 2)\rfor i, point in enumerate(points):\rprint(i, \u0026#39;-\u0026gt;\u0026#39;, point)\rhull = ConvexHull(points)\rprint(\u0026#34;The smallest convex set containing all\u0026#34;,\rnum_points, \u0026#34;points has\u0026#34;, len(hull.simplices),\r\u0026#34;sides,\\nconnecting points:\u0026#34;)\rfor simplex in hull.simplices:\rprint(simplex[0], \u0026#39;\u0026lt;-\u0026gt;\u0026#39;, simplex[1])\rreturn {\r\u0026#39;statusCode\u0026#39;: 200,\r\u0026#39;headers\u0026#39;: {\r\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;,\r\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;\r},\r\u0026#39;body\u0026#39;: \u0026#39;\u0026#39;,\r\u0026#34;isBase64Encoded\u0026#34;: False\r} This Python Lambda function will generate random data and perform its operation. It is used for experimentation and testing. Now let\u0026rsquo;s look at the role of each piece of code:\nfirst. Import Statements: The code starts by importing the necessary libraries:\njson: This library allows us to use and work with JSON data. numpy (np alias): NumPy is a library for numerical calculations in Python ConvexHull from scipy.spatial: This library provides functions for computing convex hulls, a basic concept in computational geometry. 2. lambda_handler Function: This is the main function that is executed when the Lambda function is triggered.\nms is set to 100, representing the size of the generated matrix. It prints the size of the matrix. 3. Generating and Printing Random Matrices:\nTwo random matrices, matrix_a and matrix_b, of size ms x ms are generated with random integer values below 100. These matrices are printed. The results of matrix_a and matrix_b are calculated using the dot method and printed. 4. Generating and Printing Random Points:\nnum_points is set to 10, representing the number of random points generated. It generates num_points random 2D points (points) and prints them. 5. Computing Convex Hull:\nIt calculates the convex hull of the generated random points using the function ConvexHull from scipy.spatial. It prints the number of sides of the smallest convex set containing all points and connections between points (simplices). 6. Return Statement:\nIt returns a JSON response with status code 200 and additional headers. The body of the response is empty (\u0026rsquo;\u0026rsquo;). In short, this Lambda function is used for testing and experimentation along with random data geration and convex hull computation. When triggered, it generates random matrices, randome points, and calculates the convex hull of the points, providing insights into the geometric relationships between points.\nAfter pasting the above code into the lambda_function.py file, click on the Deploy button\nAfter successful deployment, click on Configuration, then click on the Edit button.\nOn the Edit basic settings page, we will change the value of Memory to 3008, Time out is 5 min 0 sec. Then click the Save button at the bottom of the page.\nTest function Now let\u0026rsquo;s go to the Test section of the Lambda function. Name the Event name test, leave the rest as default and click the Test button.\nAfter successful testing, we will receive something like the image below.\nGo back to the top of the Lambda function page, we will see Function ARN and copy it.\n"
},
{
	"uri": "/3-graviton2/3.2-deploytest/",
	"title": "Deploy the base function",
	"tags": [],
	"description": "",
	"content": "In this section, you will deploy a computationally expensive Lambda function written in Python 3.9. Function determines the number of prime numbers between 0 and 10 million. This function is implemented in Lambda Graviton2 blog post.\nMake sure you are deploying the function in the same region as the Lambda Power Tuning Tool.\nDeploy x86 Lambda Function First, go to Lambda Console. In the left sidebar, select Functions, then click the Create function button.\nOn the Create function page, select Author from scratch. Name the Function name as lambda-base-function. Select Python 3.9 in Runtime and Architecture as x86_64. Finally, click on the Create function button.\nCopy the code below.\nimport json\rimport math\rimport platform\rimport timeit\rdef primes_up_to(n):\rprimes = []\rfor i in range(2, n+1):\ris_prime = True\rsqrt_i = math.isqrt(i)\rfor p in primes:\rif p \u0026gt; sqrt_i:\rbreak\rif i % p == 0:\ris_prime = False\rbreak\rif is_prime:\rprimes.append(i)\rreturn primes\rdef lambda_handler(event, context):\rstart_time = timeit.default_timer()\rN = 1000000\rprimes = primes_up_to(N)\rstop_time = timeit.default_timer()\relapsed_time = stop_time - start_time\rresponse = {\r\u0026#39;machine\u0026#39;: platform.machine(),\r\u0026#39;elapsed\u0026#39;: elapsed_time,\r\u0026#39;message\u0026#39;: \u0026#39;There are {} prime numbers \u0026lt;= {}\u0026#39;.format(len(primes), N)\r}\rreturn {\r\u0026#39;statusCode\u0026#39;: 200,\r\u0026#39;body\u0026#39;: json.dumps(response)\r} This Python Lambda function calculates prime numbers up to a specified N limit and measures the time required to perform this calculation. Let\u0026rsquo;s go through each part of the code:\nImport Statements: json: This library allows working with JSON data. math: This library allows working with mathematical functions. platform: This library allows access to information about the operating system, server, and Python version. timeit: This library allows measuring the execution time of a piece of code. primes_up_to function: This function calculates prime numbers up to the limit n. It initializes the empty list primes to contain good integers. It iterates over numbers from 2 to n. For each number i, it checks whether it is prime or not by dividing it by all previously found prime numbers until the square root of i. If i is divisible by any of these prime numbers then it is not prime and the loop is continuous. Otherwise, i is considered a prime number and is added to the list of primes. This function will return the list of found prime numbers. lambda_handler function: This is the Lambda function itself that will execute when the Lambda is triggered. It will measure the start execution time of the function using timeit.default_timer(). It sets the value of N to 1000000, used to find all prime numbers up to 1000000. It calls primes_up_to function to find all prime numbers up to N. It will measure the end execution time of the function using timeit.default_timer(). It includes a JSON response including: machine: type of machine or hosting environment elapsed: function execution time message: number of prime numbers found Return Statement: It returns JSON response with statusCode of 200 and body as JSON response. The body of the response includes JSON data generated in the response dictionary. In short, this Lambda function calculates prime numbers up to the limit N (in this case N=1000000), measures the time taken to execute the calculation, and provides information about the machine owner. Here\u0026rsquo;s a simple example that uses Lambda to represent a calculation task and report the results.\nAfter copying the above code, go to the lambda-base-function page, paste that code into the lambda_function.py file.\nGo to Configuration, in General configuration, click on Edit button.\nOn the Edit basic settings page, you set Timeout to 3 min 0 sec. Then click on the Save button.\nNext, going back to the Code section, we will create a test for the Lambda function by clicking on the Test button.\nWhen the Configure test event window opens, name the Event name test1. Leave the remaining items as default. Click on the Save button.\nNow, we click the Test button and see the results as shown below with status code 200.\nIncome Metrics from Lambda Power Tunning Tool First, copy the Lambda function RNA lambda-base-function.\nHãy giữ trình duyệt của Lambda function này vì chúng ta sẽ tiếp tục dùng nó.\nOpen a new browser and access AWS Step Functions console. We will see the state machine we created in the previous section. Click to select that state machine.\nCopy the script below.\n{\r\u0026#34;lambdaARN\u0026#34;: \u0026#34;YOUR LAMBDA ARN HERE\u0026#34;,\r\u0026#34;powerValues\u0026#34;: [128, 256, 512, 1024, 2048, 3008],\r\u0026#34;num\u0026#34;: 10,\r\u0026#34;payload\u0026#34;: \u0026#34;{}\u0026#34;,\r\u0026#34;parallelInvocation\u0026#34;: true,\r\u0026#34;strategy\u0026#34;: \u0026#34;cost\u0026#34;\r} Select Start execution to start analyzing the function we just created. The Start execution window opens, we paste the above script into Input, remember to replace the value of LambdaARN with the ARN we copied from the Lambda function above. Then click on the Start execution button.\nWait a moment when we see Execution status is Succeeded. And get the URL in the output similar to the Analysis section.\nNext, select the Execution input and output section. You will see that in the output there is a URL in the visualization field. Copy that URL.\nOpen a new browser (Google, CocCoc, Microsoft Edge, etc) and paste that URL. You will receive as shown below.\nPlease keep this browser intact to compare the performance of other functions in the next step.\n"
},
{
	"uri": "/4-traffic/4.2-norate/",
	"title": "No Rate Limiting",
	"tags": [],
	"description": "",
	"content": "\nIn the previous section, you deployed two separate simple architectures. The first architecture does not implement any rate limiting. The client directly calls the Lambda function to insert the payload into the DynamoDB table. Because there is no element rate limiting, DynamoDB will exceed its provisioned capacity when under heavy load.\nWe\u0026rsquo;ll explore the code in Cloud9 in a moment. First, go to the directory traffic-throttle \u0026gt; standard \u0026gt; standard_lambda_ddb and open the file app.py. Looking at it, you will see that the Lambda function is used to insert payload into the DynamoDB table.\nIn the following, we will use the load generating tool to illustrate how this architecture behaves under load.\n"
},
{
	"uri": "/2-powertuning/",
	"title": "Power Tuning",
	"tags": [],
	"description": "",
	"content": "The goal of this workshop is to learn and better understand AWS Lambda functions during the development process. Understand specifically how to configure resources for a certain Lambda function to optimize costs and performance. The goal is to understand how the execution time will change by increasing or decreasing the memory allocated to Lambda functions.\nThis workshop will use the Lambda Power Tuning Project created by Alex Casalboni. The project is located in Serverless Application Repository.\nDetails about this tool It helps you tailor memory allocation to any Lambda function deployed in your account.\nAWS Lambda Power Tuning Tool is a state machine powered by AWS Step Functions that helps you optimize Lambda functions for cost and/or performance based on data.\nState machine is designed to be easy to deploy and execute quickly. Plus, it\u0026rsquo;s an agnostic language so you can optimize every Lambda function in your account.\nYou provide the ARN of the Lambda function as input and the state machine will call that function with many different configurations (from 128MB to 3GB). It then analyzes all execution logs and makes recommendations on the best power configuration to reduce overhead and/or maximize performance.\n"
},
{
	"uri": "/2-powertuning/2.3-analysis/",
	"title": "Analysis",
	"tags": [],
	"description": "",
	"content": "You have now deployed the power tuning tool and tested the Lambda function. This is the time to find the best memory size for this function. The Power Tuning Tool will run our function with various memory sizes to help determine which configuration is best for cost and performance. In this section, we will go through the steps to analyze the configuration of our function.\nFirst, go to Step Function console. In the left sidebar, select State machines. Here we will see the state machine that you created in the previous section. Click on that state machine.\nCopy the script below.\n{\r\u0026#34;lambdaARN\u0026#34;: \u0026#34;YOUR LAMBDA ARN HERE\u0026#34;,\r\u0026#34;powerValues\u0026#34;: [128, 256, 512, 1024, 2048, 3008],\r\u0026#34;num\u0026#34;: 10,\r\u0026#34;payload\u0026#34;: \u0026#34;{}\u0026#34;,\r\u0026#34;parallelInvocation\u0026#34;: true,\r\u0026#34;strategy\u0026#34;: \u0026#34;cost\u0026#34;\r} On the powerTuningStateMachine page that has just opened, click on Start execution in the top right corner.\nWhen the Start execution page opens, paste the above script into the Input - optional section and replace YOUR LAMBDA ARN HERE with the ARN of the Lambda function you created in the ** before. Then, click on the Start execution button at the bottom of the page.\nWait until the execution is completed, you will see Execution status with the value Succeeded.\nNext, select the Execution input and output section. You will see that in the output there is a URL in the visualization field. Copy that URL.\nOpen a new browser (Google, CocCoc, Microsoft Edge, etc) and paste that URL. You will receive as shown below.\nFrom the analysis table above, you can conclude that to optimize functionality with the lowest cost, 512MB memory configuration will be best. To optimize the function for performance (ie shortest execution time), a 512MB memory configuration would be best.\nIf the original function is configured to be 128MB, increasing the memory to 512MB can reduce costs and improve performance. Increasing memory above 2048MB does not reduce execution time and therefore only leads to high costs. This shows the importance of optimizing Lambda functions in your environment.\nThis workshop illustrates the trade-off between cost and performance when creating and executing Lambda functions. Lambda Tuning can be used to evaluate resource requirements and analyze cost profiles. The Power Tuning application will be used in the Gravition module to analyze function performance on other compute architectures.\n"
},
{
	"uri": "/3-graviton2/3.3-deploygra/",
	"title": "Deploy Graviton2 Function",
	"tags": [],
	"description": "",
	"content": "In this section, you will modify the Lambda function from the previous section to deploy Graviton2 Arm-based architecture instead of x86 architecture.\nDeploy Graviton2 arm_64 Lambda Function First we need to go back to the Console of the Lambda Function created in the previous step.\nScroll down to the Runtime settings section, click the Edit button.\nOn the Edit runtime settings page, we select the arm64 architecture. Then click on the Save button.\nNow, click on the Test button and we get the output as shown.\nIncome Metrics from Lambda Power Tunning Tool Copy the RNA of Lambda function lambda-base-function. Going back to the previous step, we replace lambdaARN in the script with the copied ARN of this Lambda function. Then click on the Start execution button at the bottom of the page.\nAfter getting the URL of the output, we will get something like the picture.\nCompare performance metrics for each function To compare the results between the two architectures, click on the Compare button in the browser.\nThe information includes Name for function and Visualization URL of function.\nNow, you can see the comparison results of both Invocation Time and Cost between x86 and arm64 architecture function. Observe that the arm64 function performs better in terms of both Cost and Invocation Time in this case.\nYou can follow these steps to compare the price/performance of your own Lambda function code. Depending on the workload, you may only see a price improvement and not a performance improvement. In this case, you should take advantage of instrumentation and observability such as AWS X-Ray to identify bottlenecks. In many cases, it is the function\u0026rsquo;s dependencies that require updating, not the function\u0026rsquo;s code itself.\nBefore migrating your functions to Graviton2\nFor more instructions on migrating existing workloads to Graviton2, visit the AWS Graviton Github Repo on migrating workloads to an Amazon EC2 instance based on AWS Graviton2. Always test before making production workload changes!\n"
},
{
	"uri": "/3-graviton2/",
	"title": "Graviton2",
	"tags": [],
	"description": "",
	"content": "\nIn September 2021, [AWS announced](https://aws.amazon.com/about-aws/whats-new/2021/09/better-price-performance-aws-lambda-functions-aws-graviton2 -processor/) AWS Lambda functions will optionally be powered by AWS Graviton2 Processor, an Arm-based architecture designed and built by AWS.\nLambda functions powered by AWS Graviton2 processor can provide up to 34% better price/performance improvement compared to x86. Graviton2 functions uses an Arm-based processor architecture designed to deliver up to 19% better performance and 20% lower costs for a variety of Serverless workloads, such as web and mobile backend, data and media processing. With lower latency and better performance, functions powered by the AWS Graviton2 processor are ideal for powering mission-critical Serverless applications.\nPrice/performance improvements will vary depending on runtime and source code. In this module, we will explore testing sample code with both architectures and complete a price/performance analysis.\nTo learn more about Graviton2 on AWS, visit Getting Started on Graviton Git Repo.\nCheck out this blog post to learn more about migrating your Lambda functions to Graviton2.\n"
},
{
	"uri": "/4-traffic/4.3-rate/",
	"title": "Rate Limiting",
	"tags": [],
	"description": "",
	"content": "The second architecture deployed has rate limiting implemented using SQS Queue. In this architecture, the client will call the first Lambda function that has the payload. This Lambda function inserts records into SQS Queue. The second Lambda function will read a fixed number of messages per group from the queue and insert them into the DynamoDB table. Since we are controlling the number of messages in the pool, the throughput provided by DynamoDB (WCU \u0026amp; RCU) will not be exceeded.\nThe client will request to the Lambda function. This Lambda function inserts the payload into the SQS Queue. The second Lambda function reads a fixed number of messages in a group and inserts those items into the DynamoDB table.\nReview deployed architecture In Cloud9 workspace, go to the traffic-throttle \u0026gt; ratelimit folder folder. Here, you will see 2 folders with Lambda function code: sqs_insert_lambda and sqs_read_ddb_insert_lambda.\nLook at the file app.py in the directory sqs_insert_lambda. You will see the function sending the payload into the SQS Queue.\nThe second Lambda function, in the sqs_read_ddb_insert_lambda folder, open the app.py file, you will see the function processes messages in batch, sending the payload to the DynamoDB table.\nIn the following section, we will use the load generating tool to illustrate how this architecture works under load.\n"
},
{
	"uri": "/4-traffic/4.4-generating/",
	"title": "Generating Load",
	"tags": [],
	"description": "",
	"content": "Test Client - Artillery Installation To generate load on the two newly deployed architectures, you will use the distributed load testing tool Artillery.io. The following steps will show you how to install and use this tool in the Cloud9 environment.\nFirst, you will check if npm is installed or not with the npm -v command on Cloud9\u0026rsquo;s terminal. If you haven\u0026rsquo;t installed it yet, install both node and npm. Please follow instructions to install node and npm on your system.\nnpm -v If npm has been installed, you will see output as shown.\nNext, run the command below to install Artillery. Read Artillery.io documentation to learn more about this tool.\nnpm install -g artillery When installed successfully, we will see the image below.\nInstall Artillery Engine Lambda using the command below. References Artillery Engine Lambda.\nnpm install -g artillery-engine-lambda@1.0.18 When installed successfully, we will see the image below.\nGenerate Traffic There is a loadTest artillery file that has been included in the previously extracted traffic-throttle folder. You can test by navigating to the traffic-throttle/artillery folder and opening the LambdaTest.yaml file.\nYou will need the names of the two Lambda functions deployed at the beginning of this section. To get it, go to Lambda Console.\nFirst, let\u0026rsquo;s test the flow of non-rate limited. Copy the function name starting with traffic-throttle-StandardLambdaDDB. Then, reopen the LambdaTest.yaml file in the Cloud9 workspace, paste the function name into the target key. Change region to the region where that Lambda function is deployed. Save the file.\nStart testing by running the command below in Cloud9 terminal. Make sure you are in the path traffic-throttle/artillery. It will take about 3 to 4 minutes to complete.\nartillery run lambdaTest.yaml Now let\u0026rsquo;s test the flow of Throttle. Copy the function name starting with traffic-throttle-SQSInsertLambda and paste it into the target key. Rerun the command below to test.\nartillery run lambdaTest.yaml "
},
{
	"uri": "/4-traffic/",
	"title": "Traffic Throttling",
	"tags": [],
	"description": "",
	"content": "Generality In this section, you will deploy two architectures to describe throttling at the DynamoDB resource level.\nIn the first architecture, the client will interact with the lambda function. This Lambda function directly inserts the payload into the DynamoDB table. As traffic to the Lambda function increases, DynamoDB will consume more capacity than it has provisioned. You will see some throttled requests at the DynamoDB level.\nThe second architecture implements throttling via SQS Queue. There are two Lambda functions in this architecture. Lambda functions will first insert into the SQS Queue. The second Lambda function retrieves a fixed series of messages and inserts them into DynamoDB. The second Lambda function controls the number of messages that will be processed (throttle). This allows us to control the resources that DynamoDB (WCY and RCU) consumes and thus avoid throttling.\n"
},
{
	"uri": "/4-traffic/4.5-analysis/",
	"title": "Analysis",
	"tags": [],
	"description": "",
	"content": "DynamoDB Metrics Now, we run a load test and can check DynamoDB CloudWatch\u0026rsquo;s write metrics to observe the difference between the two solutions.\nFirst, go to DynamoDB Console to see two tables. Tale Message is used by function traffic-throttle-StandardLambdaDDB and table RateLimitMessage is used by rate limiting traffic-throttle-SQSReadDDBInsertLambda (i.e. function consumes messages from SQS Queue).\nClick on the Message table and go to the Overview section in the Table capacity metrics section shown by CloudWatch graphs. In the Write usage (average units/second) chart, click on the menu and select view in metrics to view in CloudWatch.\nIn the chart in CloudWatch, you will see capacity units consumed exceeding the supply level. This will happen because no rate limiting is applied to this Lambda function. As a result, DynamoDB\u0026rsquo;s auto scaling increased Write Capacity Units (WCU) from 12 up 37.\nNow repeat the same process to see the graph of DynamoDB table RateLimitMessage. You will see that consumed capacity does not exceed the provisioned capacity limit. Therefore, the auto scaling ability performed by DynamoDB is less dynamic, only increasing from 12 to 16.\nOverall Rate Limiting is a cheaper solution, especially when peak traffic is intermittent and occurs almost every hour. Rate Limiting limits DynamoDB from scaling resources to offset the cost of additional Lambda functions and SQS Queues. This pattern also provides modularity, separating the message creator from the message consumer.\nThis solution does not reduce costs for all request patterns.\n"
},
{
	"uri": "/5-clean/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Lambda Power Tuning Tool Please go to CloudFormation Console . Search for the keyword aws-lambda-power-tuning and select that stack. Then click on the Delete button.\nTest Lambda Function Please go to Lambda Console. Select the function you created earlier. Then click on the Actions button and select Delete.\nSAM In Cloud9\u0026rsquo;s terminal, navigate to the traffic-throttle folder.\ncd ~/environment/traffic-throttle/ Then delete using the command below.\nsam delete --stack-name traffic-throttle --no-prompts It will take a few minutes to delete. You can view the deletion process at CloudFormation Console.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]